{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression**"
      ],
      "metadata": {
        "id": "xLv0QCGtTP6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1\n",
        "What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "**Answer:**  \n",
        "Logistic Regression is a classification algorithm used to predict a categorical outcome (usually binary: 0/1, yes/no). It models the probability that an input belongs to the positive class using the logistic (sigmoid) function and outputs values between 0 and 1.\n",
        "\n",
        "**Difference from Linear Regression:**  \n",
        "- Linear Regression predicts a continuous value (e.g., price). Logistic Regression predicts probabilities for discrete classes.  \n",
        "- Linear regression uses a linear function and can produce any real number; logistic regression applies a sigmoid to the linear combination of features so outputs are constrained between 0 and 1.  \n",
        "- For classification, logistic regression’s decision boundary is based on probability (e.g., probability > 0.5 → class 1).\n",
        "\n",
        "# Question 2\n",
        "Explain the role of the Sigmoid function in Logistic Regression.\n",
        "\n",
        "**Answer:**  \n",
        "The sigmoid (logistic) function maps any real-valued input to the range (0, 1). In logistic regression we compute a linear score z = w·x + b and then apply sigmoid: σ(z) = 1 / (1 + e^(−z)). The result is interpreted as the probability of the positive class. Sigmoid makes the model output probabilities and allows thresholding (e.g., p ≥ 0.5 → class 1).\n",
        "\n",
        "# Question 3\n",
        "What is Regularization in Logistic Regression and why is it needed?\n",
        "\n",
        "**Answer:**  \n",
        "Regularization adds a penalty to the model’s loss to prevent overfitting by discouraging very large weights. Two common types:\n",
        "- **L2 (Ridge)**: penalty = λ * sum(w^2) → shrinks weights smoothly.\n",
        "- **L1 (Lasso)**: penalty = λ * sum(|w|) → can shrink some weights to zero (feature selection).\n",
        "\n",
        "Why needed: Without regularization, a model can fit noise in training data (overfit). Regularization improves generalization on unseen data.\n",
        "\n",
        "# Question 4\n",
        "What are some common evaluation metrics for classification models, and why are they important?\n",
        "\n",
        "**Answer:**  \n",
        "- **Accuracy:** (TP+TN)/Total. Good when classes balanced.  \n",
        "- **Precision:** TP / (TP + FP). Out of predicted positives, how many are correct. Important when false positives are costly.  \n",
        "- **Recall (Sensitivity):** TP / (TP + FN). Out of actual positives, how many we found. Important when missing positives is costly.  \n",
        "- **F1-score:** Harmonic mean of precision & recall — useful for imbalanced data.  \n",
        "- **ROC-AUC:** Measures separability across thresholds. Good for ranking quality.  \n",
        "- **PR-AUC (Precision-Recall AUC):** Better for highly imbalanced problems.  \n",
        "\n",
        "Choosing metrics depends on business goals (e.g., in fraud detection prioritize recall, or precision depending on cost)."
      ],
      "metadata": {
        "id": "NE_z65I7Tch1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 5\n",
        "# Load a dataset from sklearn, split into train/test, train Logistic Regression, print accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict & evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", round(acc, 4))\n",
        "\n",
        "# Short explanation:\n",
        "# We used the breast cancer dataset (binary). The printed accuracy shows model performance on test data."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6TTt_aWTljD",
        "outputId": "99dd18e5-0389-40ba-fc3b-07c47fd372ca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6\n",
        "# Train Logistic Regression with L2 regularization and print coefficients & accuracy.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Using same train/test from Q5\n",
        "model_l2 = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=1000, random_state=42)\n",
        "model_l2.fit(X_train, y_train)\n",
        "\n",
        "# Coefficients and accuracy\n",
        "coefficients = model_l2.coef_[0]\n",
        "intercept = model_l2.intercept_[0]\n",
        "y_pred_l2 = model_l2.predict(X_test)\n",
        "acc_l2 = accuracy_score(y_test, y_pred_l2)\n",
        "\n",
        "print(\"Intercept:\", round(intercept,4))\n",
        "print(\"First 10 Coefficients:\", np.round(coefficients[:10], 4))\n",
        "print(\"Accuracy (L2):\", round(acc_l2, 4))\n",
        "\n",
        "# Explanation:\n",
        "# L2 is the default regularization here. Coefficients show feature importance sign & magnitude."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABfQcPGOTnaa",
        "outputId": "05c37780-06c5-45bd-a851-f37db9e71a7d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept: 6.5806\n",
            "First 10 Coefficients: [ 2.0396  0.0391 -0.1888  0.0074 -0.2456 -0.3093 -0.7021 -0.4657 -0.4492\n",
            " -0.0065]\n",
            "Accuracy (L2): 0.9649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7\n",
        "# Multiclass classification using multi_class='ovr' and print classification report (use Iris dataset).\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "y_iris = iris.target\n",
        "\n",
        "Xtr, Xte, ytr, yte = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42, stratify=y_iris)\n",
        "\n",
        "# Train with one-vs-rest\n",
        "model_ovr = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=1000, random_state=42)\n",
        "model_ovr.fit(Xtr, ytr)\n",
        "\n",
        "y_pred_ovr = model_ovr.predict(Xte)\n",
        "print(classification_report(yte, y_pred_ovr, target_names=iris.target_names))\n",
        "\n",
        "# Explanation:\n",
        "# OVR trains one classifier per class vs rest. Classification report shows precision/recall/F1 per class."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0gDOHcXTpp6",
        "outputId": "61d0ad8f-1ef7-43a4-e26a-296e4681d636"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      0.90      0.95        10\n",
            "   virginica       0.91      1.00      0.95        10\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.97      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8\n",
        "# Use GridSearchCV to tune C and penalty (note: 'l1' requires solver that supports it like 'liblinear' or 'saga').\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# pipeline: scaling + logistic (scaler is often helpful)\n",
        "pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logreg', LogisticRegression(max_iter=1000, random_state=42, solver='liblinear'))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'logreg__C': [0.01, 0.1, 1, 10],\n",
        "    'logreg__penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "print(\"Best CV Score:\", round(grid.best_score_, 4))\n",
        "print(\"Validation Accuracy with best model:\", round(grid.score(X_test, y_test), 4))\n",
        "\n",
        "# Explanation:\n",
        "# GridSearchCV searches best C and penalty using cross-validation and returns the best parameters & score."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZycxVaMTwdQ",
        "outputId": "9c869ff1-e227-4892-fd80-63005e4ad896"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Params: {'logreg__C': 0.1, 'logreg__penalty': 'l2'}\n",
            "Best CV Score: 0.9802\n",
            "Validation Accuracy with best model: 0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # Question 9\n",
        "# Standardize features before training and compare accuracy with and without scaling.\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Without scaling\n",
        "model_no_scale = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model_no_scale.fit(X_train, y_train)\n",
        "acc_no_scale = accuracy_score(y_test, model_no_scale.predict(X_test))\n",
        "\n",
        "# With scaling (pipeline)\n",
        "pipe_scaled = Pipeline([('scaler', StandardScaler()), ('logreg', LogisticRegression(max_iter=1000, random_state=42))])\n",
        "pipe_scaled.fit(X_train, y_train)\n",
        "acc_scaled = accuracy_score(y_test, pipe_scaled.predict(X_test))\n",
        "\n",
        "print(\"Accuracy without scaling:\", round(acc_no_scale,4))\n",
        "print(\"Accuracy with scaling:\", round(acc_scaled,4))\n",
        "\n",
        "# Explanation:\n",
        "# Scaling often improves optimization and sometimes performance, especially when features have different magnitudes."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNexr81DTycK",
        "outputId": "28196ea8-1e9a-4384-81fe-57d7f0e4eedb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.9649\n",
            "Accuracy with scaling: 0.9825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10\n",
        "Imagine you are working at an e-commerce company that wants to predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% respond), describe the approach you’d take.\n",
        "\n",
        "**Answer (step-by-step practical plan):**\n",
        "\n",
        "1. **Understand data & baseline:**  \n",
        "   - Explore class balance, missing values, feature types, simple correlations.  \n",
        "   - Build a quick baseline model (logistic regression) to have a reference.\n",
        "\n",
        "2. **Data cleaning & feature engineering:**  \n",
        "   - Handle missing values (median for numeric, mode/‘missing’ for categorical).  \n",
        "   - Create useful features (recency, frequency, monetary features, interaction terms).  \n",
        "   - Convert categorical features (one-hot, target-encoding if many categories).\n",
        "\n",
        "3. **Split data properly:**  \n",
        "   - Use stratified train/test split so class distribution is preserved.\n",
        "\n",
        "4. **Address imbalance:**  \n",
        "   - Prefer **resampling on training set** only:  \n",
        "     - **SMOTE / ADASYN** (oversample minority), or  \n",
        "     - **Random undersampling** of majority, or  \n",
        "     - Use model-level solutions: **class_weight='balanced'** in LogisticRegression.  \n",
        "   - Try several approaches and validate with proper CV.\n",
        "\n",
        "5. **Feature scaling:**  \n",
        "   - Standardize numeric features (StandardScaler) if using regularized models.\n",
        "\n",
        "6. **Modeling & hyperparameter tuning:**  \n",
        "   - Use cross-validation (stratified) and tune C, penalty, solver.  \n",
        "   - Use pipelines so preprocessing + resampling + model are in a reproducible flow.\n",
        "\n",
        "7. **Use proper evaluation metrics:**  \n",
        "   - Accuracy is misleading for 5% positive. Use **Precision, Recall, F1**, and **PR-AUC**.  \n",
        "   - For business, choose metric based on cost: e.g., maximize recall if you want to reach almost all responders; maximize precision if contacting a false positive is costly.\n",
        "\n",
        "8. **Calibration & thresholding:**  \n",
        "   - Calibrate probabilities (CalibratedClassifierCV) if you need reliable probabilities.  \n",
        "   - Choose decision threshold based on business trade-offs (e.g., choose threshold to limit cost).\n",
        "\n",
        "9. **Model interpretation & monitoring:**  \n",
        "   - Check feature coefficients, SHAP or LIME for explanations.  \n",
        "   - Monitor performance on production data and retrain when distribution shifts.\n",
        "\n",
        "10. **Deployment & business integration:**  \n",
        "    - Provide predicted scores and recommended threshold.\n",
        "    - Track metrics and lift (e.g., response rate of selected customers vs baseline)."
      ],
      "metadata": {
        "id": "QtVQY2QCT4Tl"
      }
    }
  ]
}